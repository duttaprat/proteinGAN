# Experiments with hyper parameters 2018-08-16

Note: embedding size of 3 was not enough to learn M letter 
at the beginning of the sequence

#### Local tests
Setup: Running 20000 steps (each step of size 8 (8 proteins in the batch)). Number of filters 16,32,64

|Test| M | Blast (acylphosphatase) |Comments | Generated sequences|  Path|
|-----|-----|-----|-----|-----|-----|
| Learning rate from 0.00001 to 0.0001| 5,981 | 7,201 | Nothing significant. |EQMENTHISGHVQGVGLKY0SR0EDVTLR0EARTSSVIIHAQGNMAPQVQKMNNGRNEQIEA0NAD0EAVR0RVFM0NEAGGSPAVWTEIHRCN00VDRAQ0000H00000000000000000000NH |[Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\lr_0.001_b1_0.0_b2_0.999_dim_16_image_8x128_embedding) |
| Removed dilation | 7,177 (disappears later) | No info | Seems that stability decreases |QYTRVQCIIQYNGCQVGVRFALQV0LGLHGWIRNVSGRIEISGNARNVIKNGHSPDAQQQKLILHRHPGSDVFVEVHHPNSQARIRQRMNPTQH0H00H0HQ00YLR0QQ0Q0000H00H00000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\no_dilation_lr_0.0005_b1_0.0_b2_0.999_dim_16_image_8x128_embedding) |
| Wasserstein loss| 11,064 (stays till the end) |  No info | Preserves variety of proteins | MQHKQMHVNSFNQGLDDGSVEVIASMPLKLQNHLAALGIQSDG0DAIEELFIIEGQDNGEMKRVDIVKGETGFRARVTQFERHAEARIK0RSRQQ000000000000000000000000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\wasserstein\batch_size=8\d_dim_16_g_dim_16\lr_0.0005_b1_0.0_b2_0.999_dim_16_image_8x128_embedding) |
| Growing height of sequence | Never | Quite a bit of blast results, although not from the class | Seems to be not complex enough  to capture features of the protein | VLAYQKVFCVRVWGGGMVVRYKRREEMLD0IGGPGMMILEVEVDCFEITAINSAGSDYDKLLDFVDVDHSQSIFSPDEKI00DL0NSVN000000000000000000000000000000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\hinge_loss\batch_size=8\lr_0.0004_b1_0.0_b2_0.999_dim_16_image_8x128_embedding) |
| Kernel of height 8 | Never | Never | Not learning at all | 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\kernel_8_lr_0.1_b1_0.0_b2_0.999_dim_16_image_88x128_embedding) |
| Added an extra resnet layer(in block used mix dilations) |Never | Quite a bit of blast results, nothing out of required class  | Stuct with the same sequence due to perfect discriminator | DFDVDVRVTHCKCTRAMMATSDFLVTKPDVDVNDVMFICTDCRVR00KSTRVRVKVNSMKFQMMAINREMKGNPEMVIPEEMFVFPVH0LTVNCEAEEAAAAAGDVRVRPIDLNEYDVDVTAVEVDVN | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\added_extra_layer_lr_0.0004_b1_0.0_b2_0.999_dim_16_image_38x128_embedding) |
| Added an extra resnet layer(in block used mix dilations and different learning rates) | Once at 9,569 | Quite a bit of blast results, nothing out of required class| Did better job at generating different sequences, but did not perform well in general | ISSFS0NTDRISCPSWSG0DIRID0FS0TQ0SVEIFN0PIGC00VQVGQ0GNPAPID0QHVFARSPNQ0SQ00000SNNIGSWDKFVR0000000000000000000000000000000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\added_extra_layer_lr_0.001_b1_0.0_b2_0.999_dim_16_image_38x128_embedding) |
| Removed average pooling |9,569 (re-appeared multiple times)| Quite a bit of blast results, nothing out of required class | Nothing outstanding | ISSFS0NTDRISCPSWSG0DIRID0FS0TQ0SVEIFN0PIGC00VQVGQ0GNPAPID0QHVFARSPNQ0SQ00000SNNIGSWDKFVR0000000000000000000000000000000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\no_avg_pooling_lr_0.0004_b1_0.0_b2_0.999_dim_16_image_38x128_embedding)| 
| Removed average pooling and adjusted learning rates | 898 (re-appeared multiple times) | Quite a bit of blast results, nothing out of required class | Fastest and most stable in putting M at front | MMSVLEINFEFSVHSICKYIQRIMLSGK0IEISFESFEFHGQPISIFAGTVELISQRMVEAEVQSLSAQGHLETEE0GQ0LKG0LGVRWGNFNTRYY0Q00000000000000000000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\no_avg_pooling_lr_0.0002_b1_0.0_b2_0.9_dim_16_image_38x128_embedding)|
| Removed average pooling and increased std of noise to embeddings by factor of 10 | Never | Never | Did not perform well at all. Oddly even discriminator loss was 0 | 000000000000000000000000000000000000MEANENRVQGVGFYYNTR0MAEDDAQDAYCTQNSLDVTTMRNAQMAHHTYRTNINGVWGRVPGVRV0GRVHPFGVRGFI0000000000000 | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\protein_tcn\8x128_embedding\hinge_loss\batch_size=8\d_dim_16_g_dim_16\std_10_lr_0.0002_b1_0.0_b2_0.9_dim_16_image_38x128_embedding)|
| After refactoring | 300 (95% of the times M is first) | 1201 (90% of the times from correct class) | Performs significantantly better than any other architecture. Architecture was refactor, most significant changes: no same size layers, instead of reduce sum used flatten in discriminator before fully connected layer. |  | [Path](C:\Users\Donatas\Workspace\Machine Learning\SynBioAi\PREnzyme\weights\protein\wgan\mini_sample\sngan\resnet\8x128_embedding\hinge_loss\batch_size=8\d_dim_8_g_dim_8\d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_p_conv_success)|
## Conclusions
- None of that showed any positively significant changes in generated sequences.
- The main issue remains - mode collapse - which becomes apparent towards the end of 20000 steps.  
- Discriminator outperforms generator in all cases. 
- High fluctuation is all observed towards the end of 20000 steps

#### Cloud tests
Setup: Running 20000 steps (each step of size 64 (64 proteins in the batch)). Number of filters 16,32,64

|Test| M | Blast (acylphosphatase) |Comments | Generated sequences|  Path|
|-----|-----|-----|-----|-----|-----|
| Hinge loss|  5,981 from 11,064 almost 100% |2,401 - 100% till the end | Quite stable | | hinge_loss/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_p_conv |
| Wasserstein loss |  2,401 - 100% Ms appears towards the end| 6,001 (10-7) (up to 10000 consistenly from required class, after that starts deviate) | Avoids problem of Discriminator error 0 (as it can go to minus values) | MMKNN0TLQP0F0DNAMASLELQGATQEGPVNTMSSGLIQS0IGPFLSGEAQGDLENLERKIS0KCRPVRALNPVEIEPSYN0WQ0IID00NQYL000000000000000000000000000000000 |wasserstein/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_p_conv|
| Hinge loss with avg pooling | Never | 4,801 (30% of the time from correct class)| Avg pooling has negative effect on structural understanding of protein (I cannot learn M at the beginning), however it still is able to generate similar proteins (less often, less similar than with deconv) |NTDHVLIHGFVQGVGFRS0VQGLIMCQG0MMRMLERVKDGRVEAIFDGGAQVM0AMMLIMAGTGPDKSRLQIIEQHTDQKEAHVGRGRVRR0000000000000000000000000000000000000 | hinge_loss/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_p_avg |
| Hinge loss no dilation rate |  5,981 - unstable 15% of time later on | 2,401 (50% of times, did not come up in the second half of training)| No dilation has negative effect on both generating M at front and generating similar proteins |QATQTW0LTMSGRVQGIGFRDQQHDQAME0SLHGWLRSR0DGKIEVIAKSEAQTVEM0IDWLREGP0IARQNEVEGCEIRYHDEDRRFQIT0000000000000000000000000000000000000 | hinge_loss/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_1_p_conv |
| Hinge loss with stride (2,2) at the last layer of the discriminator |  898 - 100% stable | 4,801 (85% of times)| Given that model was simplified, the effect of this is relatively insignificant |MN0VCIIVFICGRVQGVGFRYQT00QAMKQGLQGYAQN0DDGSVEVVADGRNHDIE0VLKWIE0GPKRGMRIEMI0NED00NEEQQGQAHFH000000000000000000000000000000000000 | hinge_loss/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_conv  |
| Hinge loss with 7 layers of resnet blocks |  898 - 100% stable | 1,201 solid until discriminator becomes perfect | Same examples are 10-45, however, once discriminator gets perfect, generator is not doing well |MDPDDETEWYIDEQIQQGGLVEVWLMCEIETHICTSERVQQRIDEEGMEFGEEFIEEGIDEGICLFIDEIPVKIIVKGICRVPLRIVIEFVEGVTISGICPRTGKFVEFTGGIRYPRNMGDANIMPWF | hinge_loss/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0004_g_lr_0.0004_b1_0.0_b2_0.9_k_3x3_d_2_conv_n_7  |
| Hinge loss with doubled activations in each layer  | 898 - 100% stable | 1,201 - till the end of traning (getting  e value: 5.44427e-48) | There is some room for making bigger model |MSRNELDERIETYYVRVRGVVQGVGFRHATVREAHALKFDGWVANQ0DGSVEAMVQGPGA0IDRMLAWLRHGPPAARVTEVTFEERQNERRFE0F000000000000000000000000000000000 | hinge_loss/batch_size=64/d_dim_16_g_dim_16/d_lr_0.0004_g_lr_0.0004_b1_0.0_b2_0.9_k_3x3_d_2_conv_n_6  |
| Trying out subpixel upsampling | 898 - 100% stable | 1,201 - till the end of traning (getting  e value: 7.60475e-65) | Seems that subpixel upsampling helps with blast results as well as mode collapse | MSNVCIIAWVYGRVQGVGFRYTTQHEAQRLGLTGYAKNMDDGSVEVVACGDAAQVEKLIKWLKEGGPRSARVDKILTEPHSPRETLTSFSIRY00000000000000000000000000000000000| hinge_loss/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_subpixel_n_6 |
| Trying out RA hinge loss | 898 but not fully stable| Never | It seems like a step back | MNQPNIM0VSQSPVTPVWQQG0QD0CAHSLLNPTKTMNLNNPDIEVLA00FSRQAHIKK0RFRQN0ERVNVAQ0QEQQGTHRSVAEVVDVQ0000000000000000000000000000000000000| hinge_loss_ra/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_conv_n_6 |  
| Trying out subpixel upsampling with RA hinge loss |898 - 100% stable |  1,201 - till the end of traning (getting  e value: 4.09816e-66 ) | Very solid results, however, it does not seem that adding RA to the loss changes much| MSKVCIIAWVYGRVQGVGFRYTTQYEAKRLGLTGYAKNLDDGSVEVVACGEEGQVEKLMQWLKSGGPRSARVERVLSEPHHPSGELTDFRIR000000000000000000000000000000000000 | hinge_loss_ra/batch_size=64/d_dim_8_g_dim_8/d_lr_0.0002_g_lr_0.0002_b1_0.0_b2_0.9_k_3x3_d_2_subpixel_n_6 |

## Future ideas
- Dynamic learning rate (http://www.fast.ai/2018/04/30/dawnbench-fastai/)
- GAN + Autoencoder
- Come up with ideas how to make function that converts embeddings to amino acids differentiable. 
- Add learnt embeddings from another neural networks
- Replicated Inception and FID scores for proteins
- Come up with biological logical metrics.
- Progressively growing GANs
- Cycling GANs for conditional GAN
- StackGANs (smiles to protein == text to image?)