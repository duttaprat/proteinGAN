{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeeOZueJy-fW"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1522629683286,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "5zj3MnAMy-fq",
    "outputId": "85b5cede-11ac-4182-f7e5-8deffffce67b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = np.load(\"../data/cgan/full/train_features_smiles.npy\")\n",
    "val_data = np.load(\"../data/cgan/full/val_features_smiles.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"../data/classification/train_features_sample.npy\")\n",
    "val_data = np.load(\"../data/classification/val_features_sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42268, 250), (10568, 250))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.asarray([ np.asarray(element) for element in train_data[:,0]])\n",
    "val_data = np.asarray([ np.asarray(element) for element in val_data[:,0]])\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jYugVyby-g-"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1522634805178,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "41qAk9lAy-hC",
    "outputId": "a1607a69-b33e-4ef6-e792-360a4b0e250f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 3 with batches per epoch: 331\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "batches_per_epoch = int(train_data.shape[0]/batch_size)+1\n",
    "num_epochs = 3\n",
    "print(\"Number of epochs: {} with batches per epoch: {}\".format(num_epochs, batches_per_epoch))\n",
    "\n",
    "# Network Parameters\n",
    "sequence_length=train_data.shape[1]\n",
    "num_smiles_characters = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5236,
     "status": "ok",
     "timestamp": 1522636269832,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "bP37UtN5y-hG",
    "outputId": "089539a5-b6c1-4cb0-ca1c-deacde5b3cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1000)\n",
      "All parameters: 31518752.0\n",
      "Trainable parameters: 10506250\n",
      "tcn-encoder/encoded/kernel:0(5250, 1000)\n",
      "tcn-encoder/encoded/bias:0(1000,)\n",
      "decoder/decoded/kernel:0(1000, 5250)\n",
      "decoder/decoded/bias:0(5250,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    \n",
    "    with tf.variable_scope('input'):\n",
    "        sequences = tf.placeholder(tf.int32, [None, sequence_length], name='sequences')\n",
    "        is_training = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "        dataset = (tf.data.Dataset.from_tensor_slices(sequences)\n",
    "                   .shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "                   .apply(tf.contrib.data.batch_and_drop_remainder(batch_size)))\n",
    "    \n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    batch_sequences = iterator.get_next()\n",
    "    \n",
    "    with tf.variable_scope('embedding'):\n",
    "        embedded_sequences = tf.one_hot(batch_sequences, num_smiles_characters)\n",
    "\n",
    "    # Define weights\n",
    "    with tf.variable_scope('tcn-encoder'):\n",
    "        flat = tf.layers.flatten(embedded_sequences, name=\"dflat\")\n",
    "        encoded = tf.layers.dense(inputs=flat,\n",
    "                                 activation=None,\n",
    "                                 units=1000,\n",
    "                                 name=\"encoded\")\n",
    "   \n",
    "    print(encoded.shape)\n",
    "\n",
    "    with tf.variable_scope('decoder'):   \n",
    "        decoded = tf.layers.dense(inputs=encoded,\n",
    "                                 activation=None,\n",
    "                                 units=sequence_length*num_smiles_characters,\n",
    "                                 name=\"decoded\")\n",
    "        decoded = tf.reshape(decoded, \n",
    "                                             shape=[-1, sequence_length, num_smiles_characters], \n",
    "                                             name='embedded_decoded_sequences')\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope(\"loss_op\"):\n",
    "#         loss_op = tf.losses.absolute_difference(batch_sequences, decoded)\n",
    "        loss_op = tf.losses.sparse_softmax_cross_entropy(batch_sequences, decoded)\n",
    "        acc, acc_op  = tf.metrics.accuracy(labels=batch_sequences, \n",
    "                                  predictions=tf.argmax(decoded,2))\n",
    "        correct_prediction = tf.equal(tf.argmax(decoded,2, output_type=tf.int32), batch_sequences)\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"loss_op\", loss_op)\n",
    "    \n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "     # Initialize the variables (i.e. assign their default value)\n",
    "    #init = tf.global_variables_initializer()\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))\n",
    "    [ print(\"{}{}\".format(x.name, x.shape)) for x in tf.trainable_variables() if \"LayerNorm\" not in x.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(step, loss, acc):\n",
    "    print(\"Step {}, Loss={:.4f}, Accuracy={:.3f}\".format(str(step), loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):    \n",
    "    # Calculate batch loss and accuracy\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    sess.run(iterator.initializer, feed_dict={sequences: val_data})\n",
    "    while True:\n",
    "        try:\n",
    "            # Run optimization\n",
    "            loss, a = sess.run([loss_op, acc], feed_dict={is_training: False})\n",
    "            losses.append(loss)\n",
    "            accuracies.append(a)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    acc_avg = sum(accuracies)/len(accuracies)\n",
    "    print_progress(\"VALIDATION for epoch {}\".format(epoch), loss_avg, acc_avg)\n",
    "    return acc_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 416220,
     "status": "ok",
     "timestamp": 1522636686094,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "IjwOnIUmy-hM",
    "outputId": "79b8a85e-9f56-458d-ecc9-3eea13094548",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss=2.9416, Accuracy=0.201\n",
      "Step 82, Loss=1.4832, Accuracy=0.609\n",
      "Step 164, Loss=0.9955, Accuracy=0.749\n",
      "Step 246, Loss=0.7586, Accuracy=0.809\n",
      "Step VALIDATION for epoch 1, Loss=0.5568, Accuracy=0.862\n",
      "Model saved in path: ../logs/auto_encoder/v2\n",
      "Step 328, Loss=0.5076, Accuracy=0.877\n",
      "Step 410, Loss=0.3649, Accuracy=0.912\n",
      "Step 492, Loss=0.2402, Accuracy=0.943\n",
      "Step 574, Loss=0.2391, Accuracy=0.943\n",
      "Step VALIDATION for epoch 2, Loss=0.2865, Accuracy=0.927\n",
      "Model saved in path: ../logs/auto_encoder/v2\n",
      "Step 656, Loss=0.1828, Accuracy=0.959\n",
      "Step 738, Loss=0.1026, Accuracy=0.981\n",
      "Step 820, Loss=0.1209, Accuracy=0.975\n",
      "Step 902, Loss=0.0945, Accuracy=0.982\n",
      "Step VALIDATION for epoch 3, Loss=0.2203, Accuracy=0.941\n",
      "Model saved in path: ../logs/auto_encoder/v2\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random \n",
    "from datetime import datetime\n",
    "path = \"../logs/auto_encoder/\"\n",
    "log_dir = \"{}{}\".format(path, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "best_val_acc = 0.8\n",
    "sess = tf.Session(graph=graph)\n",
    "# Run the initializer\n",
    "epoch, step = 0, 0\n",
    "sess.run([init, iterator.initializer], feed_dict={sequences: train_data})\n",
    "while epoch < num_epochs:\n",
    "    try: \n",
    "        sess.run(train_op, feed_dict={is_training: True})\n",
    "        step = step +1 \n",
    "        if step % int(batches_per_epoch/4) == 0 or step == 1:\n",
    "            loss, a = sess.run([loss_op, acc], feed_dict={is_training: True})\n",
    "            print_progress(step, loss, a)\n",
    "            [s] = sess.run([summ], feed_dict={is_training: True})\n",
    "            #tb_writer.add_summary(np.mean(loss), step)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        path\n",
    "        epoch = epoch + 1\n",
    "        val_acc = validation(epoch)           \n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_path = saver.save(sess, \"{}{}\".format(path, \"v2\"))\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "        sess.run(iterator.initializer, feed_dict={sequences: train_data})\n",
    "print(\"Optimization Finished!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq = val_data[54:55,:]\n",
    "val_seq = np.repeat(val_seq, [batch_size], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 250)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([iterator.initializer], feed_dict={sequences: val_seq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_to_index = tf.argmax(decoded, axis=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoded_smiles = sess.run(decoded_to_index, feed_dict={is_training: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTSPVRILGIDPGLRRTGWGLITAQGTKLTYGDCGVVTSDGELPLALRLRELFEGIGRIVEAVRPDEVAVEETFVNKDAQATLKLGHARAMALLVPALAGLPVFEYAPNLIKKTVAGSGHAEKVQIQAMVRFLLPKAEFRVADAADALAIAITHASHRDAHALRQAHLPGGKRRSLTGQAAAGQGLAGKGFSAAAAARIEAALAKQG0000000000000000000000000000000000000000000\n",
      "MTTAVRILGIDPGLRRTGWGLIGAIGTLLRYAASGTVTSDGELDLALRLRELHEGIGRVVTAYAPDEAAVEHTFVNKDAQATLKLGAARGVALLVPALAGLPVSEYAPKLVKKTVAGTGHAEKVQIHAMVRFLLPKAEFKVADAADALAIAITHASHRPAEALAKAMARGGARRGANATAAAAVVLAGKHPSASEMAAIEAAAGKA00R00000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "from common.bio.constants import *\n",
    "print(\"\".join([ ID_TO_AMINO_ACID[acid_index] for acid_index in val_seq[0]]))\n",
    "print(\"\".join([ ID_TO_AMINO_ACID[acid_index] for acid_index in decoded_smiles[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 17 17  1 18 15  8 10  6  8  3 13  6 10 15 15 17  6 19  6 10  8  6  1\n",
      "  8  6 17 10 10 15 20  1  1 16  6 17 18 17 16  3  6  4 10  3 10  1 10 15\n",
      " 10 15  4 10  7  4  6  8  6 15 18 18 17  1 20  1 13  3  4  1  1 18  4  7\n",
      " 17  5 18 12  9  3  1 14  1 17 10  9 10  6  1  1 15  6 18  1 10 10 18 13\n",
      "  1 10  1  6 10 13 18 16  4 20  1 13  9 10 18  9  9 17 18  1  6 17  6  7\n",
      "  1  4  9 18 14  8  7  1 11 18 15  5 10 10 13  9  1  4  5  9 18  1  3  1\n",
      "  1  3  1 10  1  8  1  8 17  7  1 16  7 15 13  1  4  1 10  1  9  1 11  1\n",
      " 15  6  6  1 15 15  6  1 12  1 17  1  1  1  1 18 18 10  1  6  9  7 13 16\n",
      "  1 16  4 11  1  1  8  4  1  1  1  6  9  1  0  0 15  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(decoded_smiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 17, 16, 13, 18, 15,  8, 10,  6,  8,  3, 13,  6, 10, 15, 15, 17,\n",
       "        6, 19,  6, 10,  8, 17,  1, 14,  6, 17,  9, 10, 17, 20,  6,  3,  2,\n",
       "        6, 18, 18, 17, 16,  3,  6,  4, 10, 13, 10,  1, 10, 15, 10, 15,  4,\n",
       "       10,  5,  4,  6,  8,  6, 15,  8, 18,  4,  1, 18, 15, 13,  3,  4, 18,\n",
       "        1, 18,  4,  4, 17,  5, 18, 12,  9,  3,  1, 14,  1, 17, 10,  9, 10,\n",
       "        6,  7,  1, 15,  1, 11,  1, 10, 10, 18, 13,  1, 10,  1,  6, 10, 13,\n",
       "       18,  5,  4, 20,  1, 13, 12, 10,  8,  9,  9, 17, 18,  1,  6, 16,  6,\n",
       "        7,  1,  4,  9, 18, 14,  8, 14,  1, 11, 18, 15,  5, 10, 10, 13,  9,\n",
       "        1,  4,  5, 15, 18,  1,  3,  1,  1,  3,  1, 10,  1,  8,  1,  8, 17,\n",
       "        7,  1, 16,  7, 15,  3,  1,  7,  1, 10, 15, 14,  1,  7, 10, 13,  6,\n",
       "        6,  9, 15, 15, 16, 10, 17,  6, 14,  1,  1,  1,  6, 14,  6, 10,  1,\n",
       "        6,  9,  6,  5, 16,  1,  1,  1,  1,  1, 15,  8,  4,  1,  1, 10,  1,\n",
       "        9, 14,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "tcn_mnist.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
