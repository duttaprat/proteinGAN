{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprosed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL=\"Level_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data = np.load(\"enzyme_class_data\\\\train_features_\"+LEVEL+\".npy\")\n",
    "train_label = np.load(\"enzyme_class_data\\\\train_labels_\"+LEVEL+\".npy\")\n",
    "val_data = np.load(\"enzyme_class_data\\\\val_features_\"+LEVEL+\".npy\")\n",
    "val_label = np.load(\"enzyme_class_data\\\\val_labels_\"+LEVEL+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"saved_models/wgan_{}_v2/version1.ckpt\".format(LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((164674, 500), (164674,), (41169, 500), (41169,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_label.shape, val_data.shape, val_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_ACIDS = 21\n",
    "EMBEDDING_SIZE = 32\n",
    "NUM_CLASSES = np.amax(val_label, axis=0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH=3\n",
    "BATCH_SIZE=64\n",
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sequence(batch_size):\n",
    "    return np.random.randint(0, 20, size=(batch_size, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconvolution(x, is_training, output_shape, iteration):\n",
    "    bn = tf.layers.batch_normalization(x, training=is_training, name='bn'+str(iteration))\n",
    "    act = tf.nn.relu(bn, name='act'+str(iteration))\n",
    "    W_conv = tf.get_variable('g_wconv'+str(iteration), [5, output_shape[-1], int(act.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv = tf.get_variable('g_bconv'+str(iteration), [output_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    conv = conv1d_transpose(act, \n",
    "                            filter=W_conv, \n",
    "                            output_shape=output_shape, \n",
    "                            stride=2, \n",
    "                            padding=\"SAME\",\n",
    "                            name='conv'+str(iteration)) + b_conv\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "\n",
    "def conv1d_transpose(value, filter, output_shape, stride, padding=\"SAME\", name=None):\n",
    "    with ops.name_scope(name, \"conv1d_transpose\", [value, filter, output_shape]) as name:\n",
    "        output_shape_ = ops.convert_to_tensor(output_shape, name=\"output_shape\")\n",
    "        if not output_shape_.get_shape().is_compatible_with(tensor_shape.vector(3)):\n",
    "            raise ValueError(\"output_shape must have shape (3,), got {}\".format(output_shape_.get_shape()))\n",
    "\n",
    "    axis = 2\n",
    "   \n",
    "    if not value.get_shape()[axis].is_compatible_with(filter.get_shape()[2]):\n",
    "        raise ValueError(\"input channels does not match filter's input channels, \"\n",
    "                       \"{} != {}\".format(value.get_shape()[axis],\n",
    "                                         filter.get_shape()[2]))\n",
    "\n",
    "    if isinstance(output_shape, (list, np.ndarray)):\n",
    "      # output_shape's shape should be == [3] if reached this point.\n",
    "      if not filter.get_shape()[1].is_compatible_with(output_shape[axis]):\n",
    "        raise ValueError(\"output_shape does not match filter's output channels, {} != {}\".format(output_shape[axis],\n",
    "                              filter.get_shape()[1]))\n",
    "\n",
    "    if padding != \"VALID\" and padding != \"SAME\":\n",
    "        raise ValueError(\"padding must be either VALID or SAME: {}\".format(padding))\n",
    "\n",
    "    # Reshape the input tensor to [batch, 1, in_width, in_channels]\n",
    "    output_shape_ = array_ops.concat([output_shape_[:1], [1], output_shape_[1:]], axis=0)\n",
    "    spatial_start_dim = 1\n",
    "    strides = [1, 1, stride, 1]\n",
    "    value = array_ops.expand_dims(value, spatial_start_dim)\n",
    "    filter = array_ops.expand_dims(filter, 0)\n",
    "    result = tf.nn.conv2d_backprop_input(\n",
    "        input_sizes=output_shape_,\n",
    "        filter=filter,\n",
    "        out_backprop=value,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        data_format=\"NHWC\",\n",
    "        name=name)\n",
    "    return array_ops.squeeze(result, [spatial_start_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(sequences, is_training, reuse=False):\n",
    "\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv1d(\n",
    "            inputs=sequences,\n",
    "            filters=16,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name = \"conv1\")\n",
    "        # Convolutional Layer #2\n",
    "        conv2 = tf.layers.conv1d(\n",
    "            inputs=conv1,\n",
    "            filters=32,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name = \"conv2\")\n",
    "\n",
    "        #Batch Norm #1\n",
    "        bn3 = tf.layers.batch_normalization(conv2, training=is_training)\n",
    "\n",
    "        # Convolutional Layer #3\n",
    "        conv3 = tf.layers.conv1d(\n",
    "            inputs=bn3,\n",
    "            filters=64,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            name = \"conv3\")\n",
    "\n",
    "        #Batch Norm #2\n",
    "        bn5 = tf.layers.batch_normalization(conv3, training=is_training)\n",
    "\n",
    "        # Dense Layer\n",
    "        flat = tf.layers.flatten(bn3)\n",
    "        #dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=is_training)\n",
    "        output = tf.layers.dense(inputs=flat,\n",
    "                                 activation=None,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 bias_initializer=tf.zeros_initializer(),\n",
    "                                 units=1)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_batch, is_training, reuse=False):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()   \n",
    "        flat_conv1 = tf.layers.dense(inputs=input_batch,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 bias_initializer=tf.zeros_initializer (),\n",
    "                                 units=32)\n",
    "        conv1 = tf.reshape(flat_conv1, shape=[BATCH_SIZE, 32, 500], name='reshape')\n",
    "        print(conv1.shape)\n",
    "        conv2 = deconvolution(conv1, is_training, [BATCH_SIZE, 63, 256], 1)\n",
    "        print(conv2.shape)\n",
    "        conv3 = deconvolution(conv2, is_training, [BATCH_SIZE, 125, 128], 2)\n",
    "        print(conv3.shape)\n",
    "        conv4 = deconvolution(conv3, is_training, [BATCH_SIZE, 250, 64], 3)\n",
    "        print(conv4.shape)\n",
    "        conv5 = deconvolution(conv4, is_training, [BATCH_SIZE, 500, 32], 4)\n",
    "        print(conv5.shape)\n",
    "        act5 = tf.nn.tanh(conv5, name='act5')\n",
    "        act5 = tf.reshape(act5, shape=[BATCH_SIZE, 500, 32], name='act5_reshape')\n",
    "        return act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('input'):\n",
    "    real_sequences = tf.placeholder(tf.int32, [None, 500], name='real_sequence')\n",
    "    random_sequences = tf.placeholder(tf.int32, shape=[None, 500], name='random_sequence')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((real_sequences, random_sequences))\n",
    "dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).repeat(NUM_EPOCH)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "acid_embeddings = tf.get_variable(\"acid_embeddings\", [NUM_OF_ACIDS, EMBEDDING_SIZE])\n",
    "\n",
    "batch_real_sequences, batch_random_sequences = iterator.get_next()\n",
    "\n",
    "embedded_real_sequences = tf.nn.embedding_lookup(acid_embeddings, batch_real_sequences)\n",
    "embedded_random_sequences = tf.nn.embedding_lookup(acid_embeddings, batch_random_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 500)\n",
      "(64, 63, 256)\n",
      "(64, 125, 128)\n",
      "(64, 250, 64)\n",
      "(64, 500, 32)\n"
     ]
    }
   ],
   "source": [
    "embedded_fake_sequences = generator(embedded_random_sequences, is_training)\n",
    "logits_real = discriminator(embedded_real_sequences, is_training)\n",
    "logits_fake = discriminator(embedded_fake_sequences, is_training, reuse=True)\n",
    "# wgan-gp loss is same as wgan loss\n",
    "fake_mean = tf.reduce_mean(logits_fake)\n",
    "real_mean = tf.reduce_mean(logits_real)\n",
    "# AR d_loss = tf.reduce_mean( logits_real - logits_fake)  # This optimizes the discriminator.\n",
    "d_loss = tf.reduce_mean( logits_real - logits_fake)  # This optimizes the discriminator. #AR\n",
    "g_loss = tf.reduce_mean(-logits_fake)  # This optimizes the generator.\n",
    "\n",
    "# wgan-gp gradient panelty \n",
    "with tf.name_scope(\"Gradient_penalty\"):\n",
    "    eps = tf.random_uniform([BATCH_SIZE,1,1], minval=0.0,maxval=1.0)\n",
    "    interpolates = eps*embedded_real_sequences + (1-eps)*embedded_fake_sequences\n",
    "\n",
    "    gradients = tf.gradients(discriminator(interpolates, is_training, reuse=True), [interpolates])[0]\n",
    "    grad_norm = tf.norm(gradients[0], axis=1, ord='euclidean')\n",
    "    gradient_penalty = tf.reduce_mean(tf.square(grad_norm - 1))\n",
    "    d_loss += LAMBDA * gradient_penalty\n",
    "\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')\n",
    "# AR trainer_d = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.5, beta2=0.9).minimize(d_loss, var_list=D_vars)\n",
    "trainer_d = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.5, beta2=0.9).minimize(d_loss, var_list=D_vars) # AR\n",
    "trainer_g = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.5, beta2=0.9).minimize(g_loss, var_list=G_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64, epoch num: 3\n",
      "start training...\n",
      "steps:300, d_loss:-101.4620 ,g_loss:-106.7752 | fake_mean: 106.3761, real_mean: -4.3488\n",
      "steps:600, d_loss:-221.1001 ,g_loss:-208.0482 | fake_mean: 207.6493, real_mean: -17.9700\n",
      "steps:900, d_loss:-344.6225 ,g_loss:-310.6984 | fake_mean: 310.3452, real_mean: -36.3629\n",
      "steps:1200, d_loss:-470.0674 ,g_loss:-418.9265 | fake_mean: 418.5522, real_mean: -53.2735\n",
      "steps:1500, d_loss:-607.7389 ,g_loss:-532.8212 | fake_mean: 532.3080, real_mean: -76.7160\n",
      "steps:1800, d_loss:-745.8331 ,g_loss:-652.2019 | fake_mean: 651.7377, real_mean: -96.4648\n",
      "steps:2100, d_loss:-896.1913 ,g_loss:-776.7899 | fake_mean: 776.2832, real_mean: -120.4993\n",
      "steps:2400, d_loss:-1049.7250 ,g_loss:-906.6599 | fake_mean: 906.0986, real_mean: -145.9856\n",
      "steps:2700, d_loss:-1201.8959 ,g_loss:-1040.3845 | fake_mean: 1039.8555, real_mean: -172.7240\n",
      "steps:3000, d_loss:-1341.6388 ,g_loss:-1178.7117 | fake_mean: 1178.2125, real_mean: -195.4733\n",
      "steps:3300, d_loss:-1463.4080 ,g_loss:-1321.2192 | fake_mean: 1320.7125, real_mean: -217.6371\n",
      "steps:3600, d_loss:-1643.8180 ,g_loss:-1467.8440 | fake_mean: 1467.3413, real_mean: -259.4927\n",
      "steps:3900, d_loss:-1878.1658 ,g_loss:-1618.0010 | fake_mean: 1617.3340, real_mean: -284.3811\n",
      "steps:4200, d_loss:-1946.1217 ,g_loss:-1771.3074 | fake_mean: 1770.7891, real_mean: -297.4177\n",
      "steps:4500, d_loss:-2188.9194 ,g_loss:-1927.6512 | fake_mean: 1926.9305, real_mean: -332.7303\n",
      "steps:4800, d_loss:-2089.3308 ,g_loss:-2087.1392 | fake_mean: 2086.7800, real_mean: -362.7936\n",
      "steps:5100, d_loss:-2632.1187 ,g_loss:-2251.0918 | fake_mean: 2250.2473, real_mean: -403.8106\n",
      "steps:5400, d_loss:-2716.5581 ,g_loss:-2423.7505 | fake_mean: 2423.1355, real_mean: -421.4534\n",
      "steps:5700, d_loss:-3045.0107 ,g_loss:-2597.4688 | fake_mean: 2596.4907, real_mean: -459.9742\n",
      "steps:6000, d_loss:-3062.0425 ,g_loss:-2776.5371 | fake_mean: 2775.8159, real_mean: -490.8040\n",
      "steps:6300, d_loss:-3331.1572 ,g_loss:-2960.9590 | fake_mean: 2960.2847, real_mean: -537.4083\n",
      "steps:6600, d_loss:-3358.9541 ,g_loss:-3152.5981 | fake_mean: 3151.8372, real_mean: -552.7068\n",
      "steps:6900, d_loss:-3347.3403 ,g_loss:-3348.3240 | fake_mean: 3347.8972, real_mean: -591.1108\n",
      "steps:7200, d_loss:-4174.5352 ,g_loss:-3544.0430 | fake_mean: 3543.3210, real_mean: -653.7322\n",
      "steps:7500, d_loss:-3637.7256 ,g_loss:-3750.4849 | fake_mean: 3749.8254, real_mean: -707.2900\n",
      "Training is finished\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print (\"Batch size: {}, epoch num: {}\".format(BATCH_SIZE, NUM_EPOCH))\n",
    "print ('start training...')\n",
    "sess.run(iterator.initializer, \n",
    "         feed_dict={real_sequences: train_data, random_sequences: get_random_sequence(train_data.shape[0])})\n",
    "steps = 0\n",
    "while True:\n",
    "    try:\n",
    "        for k in range(5):\n",
    "            # Update the discriminator\n",
    "            _, dLoss, f_mean, r_mean = sess.run([trainer_d, d_loss, fake_mean, real_mean], feed_dict={is_training: True})\n",
    "            steps = steps + 1\n",
    "\n",
    "        # Update the generator, twice for good measure.\n",
    "        _, gLoss = sess.run([trainer_g, g_loss], feed_dict={is_training: True})\n",
    "        steps = steps + 1\n",
    "        if steps % 100 == 0 :\n",
    "            print ('steps:{}, d_loss:{:.4f} ,g_loss:{:.4f} | fake_mean: {:.4f}, real_mean: {:.4f}'.format(\n",
    "                steps, dLoss, gLoss, f_mean, r_mean))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print (\"Training is finished\")\n",
    "        break;\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(l):\n",
    "    return sum(l) / float(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_real = discriminator(embedded_real_sequences, is_training, reuse=True)\n",
    "val_fake = discriminator(embedded_random_sequences, is_training, reuse=True)\n",
    "val_loss = tf.reduce_mean(val_real-val_fake)\n",
    "real_predictions = tf.rint(val_real)\n",
    "fake_predictions = tf.rint(val_fake)\n",
    "correct_real_predictions = tf.equal(real_predictions, tf.zeros([BATCH_SIZE], dtype=tf.float32))\n",
    "correct_fake_predictions = tf.equal(fake_predictions, tf.ones([BATCH_SIZE], dtype=tf.float32))\n",
    "casted_real = tf.cast(correct_real_predictions, tf.float32)\n",
    "casted_fake = tf.cast(correct_fake_predictions, tf.float32)\n",
    "accuracy = (tf.reduce_mean(casted_real) + tf.reduce_mean(casted_fake))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating discriminator...\n",
      "Validation g_loss:-148.2925 ,accuracy :0.0010\n"
     ]
    }
   ],
   "source": [
    "#Validate discriminator by giving from validate data set and randomly generated\n",
    "print ('validating discriminator...')\n",
    "sess.run(iterator.initializer, \n",
    "         feed_dict={real_sequences: val_data, random_sequences: get_random_sequence(val_data.shape[0])})\n",
    "losses = []\n",
    "accuracies = []\n",
    "while True:\n",
    "    try:\n",
    "        v_loss, v_accuracy = sess.run([val_loss, accuracy], feed_dict={is_training: False})\n",
    "        losses.append(v_loss)\n",
    "        accuracies.append(v_accuracy)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print ('Validation g_loss:{:.4f} ,accuracy :{:.4f}'.format(mean(losses), mean(accuracies)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_embedding_lookup(acid_embeddings, embedded_sequence):\n",
    "    # assume embedded_chars.shape == (batch_size, length, embedding_size)\n",
    "    # acid_embeddings.shape == (vocab_size, embedding_size)\n",
    "    print (tf.nn.l2_normalize(acid_embeddings, dim=1).shape)\n",
    "    acid_embeddings_expanded = tf.tile(tf.expand_dims(acid_embeddings, axis = 0), [64, 1,1])\n",
    "    print (tf.nn.l2_normalize(acid_embeddings_expanded, dim=1).shape)\n",
    "    print (tf.nn.l2_normalize(embedded_sequence, dim=1).shape)\n",
    "    emb_distances = tf.matmul( # shape == (vocab_size, batch_size)\n",
    "        tf.nn.l2_normalize(acid_embeddings_expanded, dim=1),\n",
    "        tf.nn.l2_normalize(embedded_sequence, dim=1),\n",
    "        transpose_b=True)\n",
    "    print (emb_distances.shape)\n",
    "    return tf.argmax(emb_distances, axis=1) # shape == (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-5383bcfaef7b>:4: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "(21, 32)\n",
      "(64, 21, 32)\n",
      "(64, 500, 32)\n",
      "(64, 21, 500)\n",
      "(64, 500)\n"
     ]
    }
   ],
   "source": [
    "# Review generated examples\n",
    "sequences = reverse_embedding_lookup(acid_embeddings, embedded_fake_sequences)\n",
    "print (sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequences...\n",
      "500\n",
      "[[ 1 10 16 ...  0  0  0]\n",
      " [ 1 10 16 ...  0  0  0]\n",
      " [ 1 10 16 ...  0  0  0]\n",
      " ...\n",
      " [ 1 10 16 ...  0  0  0]\n",
      " [ 1 10 16 ...  0  0  0]\n",
      " [ 1 10 16 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print ('Generating sequences...')\n",
    "sess.run(iterator.initializer, \n",
    "         feed_dict={real_sequences: val_data[:BATCH_SIZE], random_sequences: get_random_sequence(BATCH_SIZE)})\n",
    "while True:\n",
    "    try:\n",
    "        generated_sequences = sess.run([sequences], feed_dict={is_training: False})\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print (len(generated_sequences[0][0]))\n",
    "        print (generated_sequences[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
