{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqicde8-y-fW"
   },
   "source": [
    "### Temporal Convolutional Networks Overview\n",
    "\n",
    "![TCNs](https://cdn-images-1.medium.com/max/1000/1*1cK-UEWHGaZLM-4ITCeqdQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL=\"Level_4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeeOZueJy-fW"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pretrained_emb = pd.read_csv(\"../../data/protVec_100d_3grams.csv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9048, 101)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>...</th>\n",
       "      <th>d91</th>\n",
       "      <th>d92</th>\n",
       "      <th>d93</th>\n",
       "      <th>d94</th>\n",
       "      <th>d95</th>\n",
       "      <th>d96</th>\n",
       "      <th>d97</th>\n",
       "      <th>d98</th>\n",
       "      <th>d99</th>\n",
       "      <th>d100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAA</td>\n",
       "      <td>-0.174060</td>\n",
       "      <td>-0.095756</td>\n",
       "      <td>0.059515</td>\n",
       "      <td>0.039673</td>\n",
       "      <td>-0.375934</td>\n",
       "      <td>-0.115415</td>\n",
       "      <td>0.090725</td>\n",
       "      <td>0.173422</td>\n",
       "      <td>0.292520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244482</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.137528</td>\n",
       "      <td>0.138140</td>\n",
       "      <td>0.005474</td>\n",
       "      <td>0.070719</td>\n",
       "      <td>-0.164084</td>\n",
       "      <td>-0.179274</td>\n",
       "      <td>0.184899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALA</td>\n",
       "      <td>-0.114085</td>\n",
       "      <td>-0.093288</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>-0.037351</td>\n",
       "      <td>-0.121446</td>\n",
       "      <td>0.084037</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.093442</td>\n",
       "      <td>0.143256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075584</td>\n",
       "      <td>-0.139661</td>\n",
       "      <td>0.034863</td>\n",
       "      <td>0.056078</td>\n",
       "      <td>0.028975</td>\n",
       "      <td>-0.012233</td>\n",
       "      <td>0.059669</td>\n",
       "      <td>0.037811</td>\n",
       "      <td>-0.172493</td>\n",
       "      <td>0.074655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLL</td>\n",
       "      <td>-0.075594</td>\n",
       "      <td>-0.100834</td>\n",
       "      <td>-0.046616</td>\n",
       "      <td>-0.208980</td>\n",
       "      <td>-0.008596</td>\n",
       "      <td>-0.038612</td>\n",
       "      <td>-0.049360</td>\n",
       "      <td>0.060720</td>\n",
       "      <td>-0.062662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174677</td>\n",
       "      <td>-0.175961</td>\n",
       "      <td>-0.193242</td>\n",
       "      <td>-0.072965</td>\n",
       "      <td>-0.075560</td>\n",
       "      <td>0.158286</td>\n",
       "      <td>-0.026378</td>\n",
       "      <td>0.037155</td>\n",
       "      <td>-0.176038</td>\n",
       "      <td>0.319293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LAA</td>\n",
       "      <td>-0.137546</td>\n",
       "      <td>-0.135425</td>\n",
       "      <td>0.121566</td>\n",
       "      <td>-0.038295</td>\n",
       "      <td>-0.212129</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.078545</td>\n",
       "      <td>0.029837</td>\n",
       "      <td>0.138343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133947</td>\n",
       "      <td>-0.156484</td>\n",
       "      <td>-0.048541</td>\n",
       "      <td>0.141848</td>\n",
       "      <td>0.081842</td>\n",
       "      <td>0.070573</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.035281</td>\n",
       "      <td>-0.138971</td>\n",
       "      <td>0.105997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAL</td>\n",
       "      <td>-0.156112</td>\n",
       "      <td>-0.133524</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>-0.058513</td>\n",
       "      <td>0.057005</td>\n",
       "      <td>0.076881</td>\n",
       "      <td>0.054781</td>\n",
       "      <td>0.129436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154597</td>\n",
       "      <td>-0.050440</td>\n",
       "      <td>0.054866</td>\n",
       "      <td>0.066185</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>-0.083944</td>\n",
       "      <td>-0.003867</td>\n",
       "      <td>-0.106367</td>\n",
       "      <td>0.070706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  words        d1        d2        d3        d4        d5        d6        d7  \\\n",
       "0   AAA -0.174060 -0.095756  0.059515  0.039673 -0.375934 -0.115415  0.090725   \n",
       "1   ALA -0.114085 -0.093288  0.155800 -0.037351 -0.121446  0.084037  0.023819   \n",
       "2   LLL -0.075594 -0.100834 -0.046616 -0.208980 -0.008596 -0.038612 -0.049360   \n",
       "3   LAA -0.137546 -0.135425  0.121566 -0.038295 -0.212129  0.040009  0.078545   \n",
       "4   AAL -0.156112 -0.133524  0.114426 -0.020264 -0.058513  0.057005  0.076881   \n",
       "\n",
       "         d8        d9    ...          d91       d92       d93       d94  \\\n",
       "0  0.173422  0.292520    ...     0.244482  0.015974  0.012903  0.137528   \n",
       "1  0.093442  0.143256    ...     0.075584 -0.139661  0.034863  0.056078   \n",
       "2  0.060720 -0.062662    ...     0.174677 -0.175961 -0.193242 -0.072965   \n",
       "3  0.029837  0.138343    ...     0.133947 -0.156484 -0.048541  0.141848   \n",
       "4  0.054781  0.129436    ...     0.154597 -0.050440  0.054866  0.066185   \n",
       "\n",
       "        d95       d96       d97       d98       d99      d100  \n",
       "0  0.138140  0.005474  0.070719 -0.164084 -0.179274  0.184899  \n",
       "1  0.028975 -0.012233  0.059669  0.037811 -0.172493  0.074655  \n",
       "2 -0.075560  0.158286 -0.026378  0.037155 -0.176038  0.319293  \n",
       "3  0.081842  0.070573  0.006927  0.035281 -0.138971  0.105997  \n",
       "4  0.017498  0.001773 -0.083944 -0.003867 -0.106367  0.070706  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights_array = pretrained_emb.drop(\"words\", axis = 1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1522629683286,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "5zj3MnAMy-fq",
    "outputId": "85b5cede-11ac-4182-f7e5-8deffffce67b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load(\"../../data/emb_train_features_\"+LEVEL+\".npy\")\n",
    "train_label = np.load(\"../../data/emb_train_labels_\"+LEVEL+\".npy\")\n",
    "val_data = np.load(\"../../data/emb_val_features_\"+LEVEL+\".npy\")\n",
    "val_label = np.load(\"../../data/emb_val_labels_\"+LEVEL+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_x00hIey-fw"
   },
   "source": [
    "## Building TCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcFQu3F0y-fy"
   },
   "source": [
    "###  Causal Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByuyggHey-gI"
   },
   "source": [
    "###  Spatial Dropout\n",
    "\n",
    "Reference: https://stats.stackexchange.com/questions/282282/how-is-spatial-dropout-in-2d-implemented\n",
    "\n",
    "Actually, simply setting noise_shape in tf.layers.Dropout will do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1522629692360,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "YRTsgwSGy-gK",
    "outputId": "6def0a04-cf65-4226-e6c9-4f7630ae02cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4, 10)\n",
      "[[ 0.          0.         -0.         -0.         -0.         -0.\n",
      "   0.         -0.         -0.          0.        ]\n",
      " [ 0.          0.         -0.          0.         -0.          0.\n",
      "   0.          0.         -0.         -0.        ]\n",
      " [ 1.4055752  -1.7585988  -3.4812396   2.007582   -3.5614364  -2.109531\n",
      "  -2.0887272  -4.7906737  -2.1057696   0.18137601]\n",
      " [ 1.4158698  -3.69789    -1.4131063  -2.2818372  -1.8009049  -0.95027846\n",
      "   2.6366224   2.420438   -0.96060276  1.865662  ]]\n",
      "[[ 0.9109896  -0.40115687 -3.40389    -2.098538   -2.512576   -3.6847801\n",
      "   0.6041635   0.12205655  1.3454405   0.8392874 ]\n",
      " [ 0.         -0.         -0.         -0.         -0.          0.\n",
      "   0.          0.         -0.          0.        ]\n",
      " [ 0.         -0.          0.         -0.         -0.          0.\n",
      "  -0.         -0.         -0.          0.        ]\n",
      " [-0.          0.          0.         -0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 4, 10)) # (batch_size, channel, length)\n",
    "    dropout = tf.layers.Dropout(0.5, noise_shape=[x.shape[0], x.shape[1], tf.constant(1)])\n",
    "    output = dropout(x, training=True)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output)\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, :])\n",
    "    print(res[1, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdTffJqQy-gU"
   },
   "source": [
    "### Temporal blocks\n",
    "\n",
    "Note: `tf.contrib.layers.layer_norm` only supports `channels_last`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9bHYwhRxy-gW"
   },
   "outputs": [],
   "source": [
    "# Redefining CausalConv1D to simplify its return values\n",
    "class CausalConv1D(tf.layers.Conv1D):\n",
    "    def __init__(self, filters,\n",
    "               kernel_size,\n",
    "               strides=1,\n",
    "               dilation_rate=1,\n",
    "               activation=None,\n",
    "               use_bias=True,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=tf.zeros_initializer(),\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               trainable=True,\n",
    "               name=None,\n",
    "               **kwargs):\n",
    "        super(CausalConv1D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            trainable=trainable,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
    "        inputs = tf.pad(inputs, tf.constant([(0, 0,), (1, 0), (0, 0)]) * padding)\n",
    "        return super(CausalConv1D, self).call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "re2siwPgy-ga"
   },
   "outputs": [],
   "source": [
    "class TemporalBlock(tf.layers.Layer):\n",
    "    def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.2, \n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalBlock, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )        \n",
    "        self.dropout = dropout\n",
    "        self.n_outputs = n_outputs\n",
    "        self.conv1 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv1\")\n",
    "        self.conv2 = CausalConv1D(\n",
    "            n_outputs, kernel_size, strides=strides, \n",
    "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
    "            name=\"conv2\")\n",
    "        self.down_sample = None\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channel_dim = 2\n",
    "        self.dropout1 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        self.dropout2 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
    "        if input_shape[channel_dim] != self.n_outputs:\n",
    "            # self.down_sample = tf.layers.Conv1D(\n",
    "            #     self.n_outputs, kernel_size=1, \n",
    "            #     activation=None, data_format=\"channels_last\", padding=\"valid\")\n",
    "            self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.contrib.layers.layer_norm(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        if self.down_sample is not None:\n",
    "            inputs = self.down_sample(inputs)\n",
    "        return tf.nn.relu(x + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1162,
     "status": "ok",
     "timestamp": 1522634695536,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "-CMZKL1jy-ge",
    "outputId": "05e31b54-3bfa-4049-f538-a20844b15f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donatasrep/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From /home/donatasrep/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "(32, 10, 8)\n",
      "[0.         0.00986617 1.1228504  0.         0.         0.08626242\n",
      " 0.23798266 0.         0.162916   0.2332388 ]\n",
      "[0.5366014  0.2239842  0.81678814 0.         0.75807244 0.95753396\n",
      " 0.         0.09215485 0.         0.42887744]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
    "    tblock = TemporalBlock(8, 2, 1, 4) #n_outputs, kernel_size, strides, dilation_rate\n",
    "    output = tblock(x, training=tf.constant(True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output)\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, 0])\n",
    "    print(res[1, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhzHzZtMy-gk"
   },
   "source": [
    "### Temporal convolutional networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GeLztD1Ly-gm"
   },
   "outputs": [],
   "source": [
    "class TemporalConvNet(tf.layers.Layer):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
    "                 trainable=True, name=None, dtype=None, \n",
    "                 activity_regularizer=None, **kwargs):\n",
    "        super(TemporalConvNet, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs\n",
    "        )\n",
    "        self.layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            out_channels = num_channels[i]\n",
    "            self.layers.append(\n",
    "                TemporalBlock(out_channels, kernel_size, strides=1, dilation_rate=dilation_size,\n",
    "                              dropout=dropout, name=\"tblock_{}\".format(i))\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        outputs = inputs\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs, training=training)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1672,
     "status": "ok",
     "timestamp": 1522634708682,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "cRTtl0mey-go",
    "outputId": "e335ba5a-34b1-453b-fea7-224fa3f7c606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 8)\n",
      "[0.95289564 0.         0.         2.019168   3.286333   2.2514975\n",
      " 9.05715    0.         3.7902834  0.80918753]\n",
      "[0.32888985 1.3946958  0.         0.20940956 0.         0.\n",
      " 4.659095   0.         4.667108   3.3278246 ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
    "    tcn = TemporalConvNet([8, 8, 8, 8], 2, 0.25)\n",
    "    output = tcn(x, training=tf.constant(True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output)\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, 0])\n",
    "    print(res[1, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1728,
     "status": "ok",
     "timestamp": 1522634710620,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "zCMhWfWjy-g0",
    "outputId": "9d39a675-debb-41e0-86e5-970c57845079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 8)\n",
      "[0.         0.         2.273004   1.6866876  0.28753886 0.\n",
      " 6.327194   0.         2.7823796  2.7996778 ]\n",
      "[1.2137653 0.        0.        0.        0.        0.        0.3347146\n",
      " 0.        0.        3.073979 ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    Xinput = tf.placeholder(tf.float32, shape=[None, 10, 4])\n",
    "    tcn = TemporalConvNet([8, 8, 8, 8], 2, 0.25)\n",
    "    output = tcn(Xinput, training=tf.constant(True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    res = sess.run(output, {Xinput: np.random.randn(32, 10, 4)})\n",
    "    print(res.shape)   \n",
    "    print(res[0, :, 0])\n",
    "    print(res[1, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jYugVyby-g-"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1522634805178,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "41qAk9lAy-hC",
    "outputId": "a1607a69-b33e-4ef6-e792-360a4b0e250f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 12 with batches per epoch: 1245\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "batches_per_epoch = int(train_data.shape[0]/batch_size)+1\n",
    "num_epochs = 12\n",
    "print(\"Number of epochs: {} with batches per epoch: {}\".format(num_epochs, batches_per_epoch))\n",
    "\n",
    "# Network Parameters\n",
    "sequence_length=train_data.shape[1]\n",
    "num_classes = np.amax(val_label, axis=0)+1 \n",
    "num_of_kmer = len(embedding_weights_array)\n",
    "embedding_size = len(embedding_weights_array[0])\n",
    "\n",
    "dropout = 0.1\n",
    "kernel_size = 3\n",
    "levels = 6\n",
    "nhid = 64 # hidden layer num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5236,
     "status": "ok",
     "timestamp": 1522636269832,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "bP37UtN5y-hG",
    "outputId": "089539a5-b6c1-4cb0-ca1c-deacde5b3cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters: 1644785.0\n",
      "Trainable parameters: 246661\n",
      "tcn/temporal_conv_net/tblock_0/conv1/kernel:0(3, 100, 64)\n",
      "tcn/temporal_conv_net/tblock_0/conv1/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_0/conv2/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_0/conv2/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_0/dense/kernel:0(100, 64)\n",
      "tcn/temporal_conv_net/tblock_0/dense/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_1/conv1/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_1/conv1/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_1/conv2/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_1/conv2/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_2/conv1/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_2/conv1/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_2/conv2/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_2/conv2/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_3/conv1/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_3/conv1/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_3/conv2/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_3/conv2/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_4/conv1/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_4/conv1/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_4/conv2/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_4/conv2/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_5/conv1/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_5/conv1/bias:0(64,)\n",
      "tcn/temporal_conv_net/tblock_5/conv2/kernel:0(3, 64, 64)\n",
      "tcn/temporal_conv_net/tblock_5/conv2/bias:0(64,)\n",
      "tcn/dense/kernel:0(64, 1285)\n",
      "tcn/dense/bias:0(1285,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf.set_random_seed(10)\n",
    "    \n",
    "    with tf.variable_scope('input'):\n",
    "        sequences = tf.placeholder(tf.int32, [None, sequence_length], name='sequences')\n",
    "        labels = tf.placeholder(tf.int32, (None,))\n",
    "        is_training = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "        dataset = (tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "                   .shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "                   .apply(tf.contrib.data.batch_and_drop_remainder(batch_size)))\n",
    "    \n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('embedding'):\n",
    "        weights_initializer = tf.constant_initializer(embedding_weights_array)\n",
    "        embedding_weights = tf.get_variable(\n",
    "            name='embedding_weights', \n",
    "            shape=(num_of_kmer, embedding_size), \n",
    "            initializer=weights_initializer,\n",
    "            trainable=False)\n",
    "#         acid_embeddings = tf.get_variable(\"acid_embeddings\", [num_of_acids, embedding_size])\n",
    "\n",
    "        batch_sequences, batch_labels = iterator.get_next()\n",
    "\n",
    "        embedded_sequences = tf.nn.embedding_lookup(embedding_weights, batch_sequences)\n",
    "#         embedded_sequences = tf.nn.embedding_lookup(acid_embeddings, batch_sequences)\n",
    "        embedded_sequences = tf.reshape(embedded_sequences, \n",
    "                                             shape=[-1, sequence_length, embedding_size], \n",
    "                                             name='embedded_real_sequences')    \n",
    "    # Define weights\n",
    "    with tf.variable_scope('tcn'):\n",
    "        logits = tf.layers.dense(TemporalConvNet([nhid] * levels, kernel_size, \n",
    "                                                 dropout)(embedded_sequences, training=is_training)[:, -1, :],\n",
    "            num_classes, activation=None, kernel_initializer=tf.orthogonal_initializer())\n",
    "   \n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope(\"loss_op\"):\n",
    "        loss_op = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=batch_labels, logits=logits))\n",
    "        tf.summary.scalar(\"loss_op\", loss_op)\n",
    "    \n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, 1, output_type=tf.int32), tf.squeeze(batch_labels))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "     # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
    "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))\n",
    "    [ print(\"{}{}\".format(x.name, x.shape)) for x in tf.trainable_variables() if \"LayerNorm\" not in x.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(step, loss, acc):\n",
    "    print(\"Step {}, Loss={:.4f}, Accuracy={:.3f}\".format(str(step), loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):    \n",
    "    # Calculate batch loss and accuracy\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    sess.run(iterator.initializer, feed_dict={sequences: val_data, labels: val_label})\n",
    "    while True:\n",
    "        try:\n",
    "            # Run optimization\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={is_training: False})\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    acc_avg = sum(accuracies)/len(accuracies)\n",
    "    print_progress(\"VALIDATION for epoch {}\".format(epoch), loss_avg, acc_avg)\n",
    "    return acc_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 416220,
     "status": "ok",
     "timestamp": 1522636686094,
     "user": {
      "displayName": "CeShine Lee",
      "photoUrl": "//lh6.googleusercontent.com/-TKaCzeGtBXw/AAAAAAAAAAI/AAAAAAAAjB4/Xqwbek0CNps/s50-c-k-no/photo.jpg",
      "userId": "114938319508229761672"
     },
     "user_tz": -480
    },
    "id": "IjwOnIUmy-hM",
    "outputId": "79b8a85e-9f56-458d-ecc9-3eea13094548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss=7.2150, Accuracy=0.008\n",
      "Step 311, Loss=5.6659, Accuracy=0.047\n",
      "Step 622, Loss=4.7612, Accuracy=0.078\n",
      "Step 933, Loss=4.2461, Accuracy=0.180\n",
      "Step VALIDATION for epoch 1, Loss=4.2032, Accuracy=0.173\n",
      "Step 1244, Loss=4.3437, Accuracy=0.164\n",
      "Step 1555, Loss=4.0910, Accuracy=0.188\n",
      "Step 1866, Loss=4.0806, Accuracy=0.211\n",
      "Step 2177, Loss=3.8330, Accuracy=0.188\n",
      "Step VALIDATION for epoch 2, Loss=3.4746, Accuracy=0.277\n",
      "Step 2488, Loss=3.5093, Accuracy=0.273\n",
      "Step 2799, Loss=4.6315, Accuracy=0.180\n",
      "Step 3110, Loss=3.5563, Accuracy=0.242\n",
      "Step 3421, Loss=3.3471, Accuracy=0.289\n",
      "Step VALIDATION for epoch 3, Loss=2.9640, Accuracy=0.367\n",
      "Step 3732, Loss=3.2815, Accuracy=0.289\n",
      "Step 4043, Loss=3.1671, Accuracy=0.312\n",
      "Step 4354, Loss=2.9249, Accuracy=0.383\n",
      "Step 4665, Loss=3.1327, Accuracy=0.273\n",
      "Step VALIDATION for epoch 4, Loss=2.8107, Accuracy=0.400\n",
      "Step 4976, Loss=2.8315, Accuracy=0.336\n",
      "Step 5287, Loss=2.8589, Accuracy=0.375\n",
      "Step 5598, Loss=3.2706, Accuracy=0.305\n",
      "Step 5909, Loss=2.3473, Accuracy=0.492\n",
      "Step VALIDATION for epoch 5, Loss=2.7094, Accuracy=0.419\n",
      "Step 6220, Loss=2.6751, Accuracy=0.445\n",
      "Step 6531, Loss=2.7982, Accuracy=0.406\n",
      "Step 6842, Loss=2.7579, Accuracy=0.414\n",
      "Step 7153, Loss=2.8895, Accuracy=0.359\n",
      "Step VALIDATION for epoch 6, Loss=2.5312, Accuracy=0.452\n",
      "Step 7464, Loss=2.1656, Accuracy=0.516\n",
      "Step 7775, Loss=2.5070, Accuracy=0.422\n",
      "Step 8086, Loss=2.5557, Accuracy=0.422\n",
      "Step 8397, Loss=2.3686, Accuracy=0.453\n",
      "Step VALIDATION for epoch 7, Loss=2.3853, Accuracy=0.490\n",
      "Step 8708, Loss=2.3376, Accuracy=0.406\n",
      "Step 9019, Loss=2.8112, Accuracy=0.344\n",
      "Step 9330, Loss=2.3327, Accuracy=0.430\n",
      "Step 9641, Loss=2.3700, Accuracy=0.500\n",
      "Step VALIDATION for epoch 8, Loss=2.2452, Accuracy=0.518\n",
      "Step 9952, Loss=2.4385, Accuracy=0.500\n",
      "Step 10263, Loss=2.5294, Accuracy=0.469\n",
      "Step 10574, Loss=2.5619, Accuracy=0.438\n",
      "Step 10885, Loss=1.8338, Accuracy=0.609\n",
      "Step VALIDATION for epoch 9, Loss=2.0807, Accuracy=0.559\n",
      "Step 11196, Loss=2.4263, Accuracy=0.453\n",
      "Step 11507, Loss=2.1385, Accuracy=0.516\n",
      "Step 11818, Loss=1.8326, Accuracy=0.602\n",
      "Step 12129, Loss=2.5762, Accuracy=0.422\n",
      "Step VALIDATION for epoch 10, Loss=2.0159, Accuracy=0.572\n",
      "Step 12440, Loss=2.0483, Accuracy=0.492\n",
      "Step 12751, Loss=2.3558, Accuracy=0.484\n",
      "Step 13062, Loss=1.8840, Accuracy=0.539\n",
      "Step 13373, Loss=2.0093, Accuracy=0.531\n",
      "Step VALIDATION for epoch 11, Loss=1.9101, Accuracy=0.599\n",
      "Step 13684, Loss=2.4700, Accuracy=0.453\n",
      "Step 13995, Loss=1.8517, Accuracy=0.578\n",
      "Step 14306, Loss=1.9055, Accuracy=0.523\n",
      "Step 14617, Loss=1.7822, Accuracy=0.570\n",
      "Step VALIDATION for epoch 12, Loss=1.8514, Accuracy=0.613\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random \n",
    "from datetime import datetime\n",
    "path = \"../../logs/tcn_sequence/\"\n",
    "log_dir = \"{}{}\".format(path, datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "best_val_acc = 0.8\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Run the initializer\n",
    "    epoch, step = 0, 0\n",
    "    sess.run([init, iterator.initializer], feed_dict={sequences: train_data, labels: train_label})\n",
    "    while epoch < num_epochs:\n",
    "        try: \n",
    "            sess.run(train_op, feed_dict={is_training: True})\n",
    "            step = step +1 \n",
    "            if step % int(batches_per_epoch/4) == 0 or step == 1:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={is_training: True})\n",
    "                print_progress(step, loss, acc)\n",
    "                [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={is_training: True})\n",
    "                tb_writer.add_summary(s, step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            path\n",
    "            epoch = epoch + 1\n",
    "            val_acc = validation(epoch)           \n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                save_path = saver.save(sess, \"{}{}\".format(path, \"v2\"))\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "            sess.run(iterator.initializer, feed_dict={sequences: train_data, labels: train_label})\n",
    "    print(\"Optimization Finished!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with new sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 500)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"..//..//data//test_sequences.csv\", sep='\\t', skipinitialspace=True)\n",
    "data[\"Sequence\"] = data.Sequence.str.ljust(500, '0')\n",
    "letterToIndex = {'0': 0, 'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12,\n",
    "                 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}\n",
    "data[\"Sequence_vector\"] = [[letterToIndex[char] for char in val ] for index, val in data.Sequence.iteritems()]\n",
    "test_data= np.asarray([ np.asarray(element) for element in data[\"Sequence_vector\"].values])\n",
    "test_data_for_tensorflow = np.append(test_data, np.zeros((batch_size-len(test_data), sequence_length)), axis=0).astype(int)\n",
    "test_data_for_tensorflow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_elements = np.array([1,2,3,4,5,6])\n",
    "label_for_tensorflow = np.append(test_elements, np.zeros((batch_size-len(test_elements))), axis=0).astype(int)\n",
    "label_for_tensorflow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../logs/tcn_sequence/v1\n"
     ]
    }
   ],
   "source": [
    "s = tf.Session(graph=graph)\n",
    "s.run(init)\n",
    "saver.restore(s, \"../logs/tcn_sequence/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\n",
      "Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases\n",
      "[[[0.         0.         0.05265274 ... 0.00000001 0.00000001 0.        ]]]\n",
      "23\n",
      "[[[0.        0.        0.0001981 ... 0.        0.        0.       ]]]\n",
      "695\n",
      "[[[0.         0.         0.21845001 ... 0.00000001 0.00000011 0.        ]]]\n",
      "23\n",
      "[[[0.         0.         0.00677673 ... 0.         0.         0.        ]]]\n",
      "695\n",
      "[[[0.         0.         0.06336619 ... 0.00000002 0.         0.        ]]]\n",
      "23\n",
      "[[[0.         0.         0.00001192 ... 0.         0.         0.        ]]]\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=8)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "s.run(iterator.initializer, feed_dict={sequences: test_data_for_tensorflow, labels: label_for_tensorflow})\n",
    "preds, ls = s.run([prediction, batch_labels], feed_dict={is_training: False})\n",
    "count = 0\n",
    "selected_ls = ls.take(np.argwhere(ls > 0))\n",
    "selected_preds = preds.take(np.argwhere(ls > 0), axis=0)\n",
    "for i in selected_ls.argsort(axis=0):\n",
    "    if count == 0:\n",
    "        print(\"\\n\\r\")\n",
    "        print(\"Oxidoreductases Transferases Hydrolases Lyases Isomerases Ligases\")\n",
    "    print(selected_preds[i])\n",
    "    print(np.argmax(selected_preds[i]))\n",
    "    count = count + 1\n",
    "\n",
    "#     print( p[\"classes\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [4],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_ls.argsort(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [4],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5], dtype=int32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.sort()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123],\n",
       "       [124],\n",
       "       [125],\n",
       "       [126],\n",
       "       [127]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(ls > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "tcn_mnist.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
