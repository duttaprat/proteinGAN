{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preprosed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL=\"Level_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data = np.load(\"train_features_\"+LEVEL+\".npy\")\n",
    "train_label = np.load(\"train_labels_\"+LEVEL+\".npy\")\n",
    "val_data = np.load(\"val_features_\"+LEVEL+\".npy\")\n",
    "val_label = np.load(\"val_labels_\"+LEVEL+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159709, 500), (159709,), (39928, 500), (39928,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_label.shape, val_data.shape, val_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_ACIDS = 21\n",
    "EMBEDDING_SIZE = 32\n",
    "NUM_CLASSES = np.amax(val_label, axis=0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_placeholder = tf.placeholder(tf.int32, (None, train_data.shape[1]))\n",
    "labels_placeholder = tf.placeholder(tf.int32, (None,))\n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(64)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: train_data, labels_placeholder: train_label})\n",
    "\n",
    "acid_embeddings = tf.get_variable(\"acid_embeddings\", [NUM_OF_ACIDS, EMBEDDING_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features, batch_labels = iterator.get_next()\n",
    "embedded_acids = tf.nn.embedding_lookup(acid_embeddings, batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_acids_flatten = tf.layers.flatten(embedded_acids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv1d(\n",
    "  inputs=embedded_acids,\n",
    "  filters=32,\n",
    "  kernel_size=5,\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.selu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "pool1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2)\n",
    "\n",
    "# Convolutional Layer #2 and Pooling Layer #2\n",
    "conv2 = tf.layers.conv1d(\n",
    "  inputs=pool1,\n",
    "  filters=64,\n",
    "  kernel_size=5,\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.selu)\n",
    "pool2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2)\n",
    "\n",
    "# Dense Layer\n",
    "pool2_flat = tf.layers.flatten(pool2)\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.selu)\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=is_training)\n",
    "\n",
    "# Logits Layer\n",
    "x = tf.layers.dense(inputs=dropout, units=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.losses.sparse_softmax_cross_entropy(labels=batch_labels, logits=x)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(x, 1, output_type=tf.int32), tf.squeeze(batch_labels))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(step, loss, acc):\n",
    "    print(\"Step {}, Minibatch Loss={:.4f}, Training Accuracy={:.3f}\".format(str(step), loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(sess, val_data, val_label):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    sess.run(iterator.initializer, feed_dict={features_placeholder: val_data, labels_placeholder: val_label})\n",
    "    while True:\n",
    "        try:\n",
    "            # Run optimization\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={is_training: False})\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print (\"Validation dataset is over\")\n",
    "            break\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    acc_avg = sum(accuracies)/len(accuracies)\n",
    "    print_progress(\"VALIDATION_STEP\", loss_avg, acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss=7.8345, Training Accuracy=0.031\n",
      "Step 100, Minibatch Loss=2.6947, Training Accuracy=0.578\n",
      "Step 200, Minibatch Loss=2.0440, Training Accuracy=0.656\n",
      "Step 300, Minibatch Loss=1.1548, Training Accuracy=0.797\n",
      "Step 400, Minibatch Loss=1.3773, Training Accuracy=0.781\n",
      "Step 500, Minibatch Loss=1.3851, Training Accuracy=0.766\n",
      "Step 600, Minibatch Loss=0.6354, Training Accuracy=0.891\n",
      "Step 700, Minibatch Loss=1.1600, Training Accuracy=0.812\n",
      "Step 800, Minibatch Loss=1.5294, Training Accuracy=0.797\n",
      "Step 900, Minibatch Loss=0.8012, Training Accuracy=0.859\n",
      "Step 1000, Minibatch Loss=1.1610, Training Accuracy=0.812\n",
      "Step 1100, Minibatch Loss=0.3635, Training Accuracy=0.891\n",
      "Step 1200, Minibatch Loss=1.1254, Training Accuracy=0.797\n",
      "Step 1300, Minibatch Loss=0.5955, Training Accuracy=0.906\n",
      "Step 1400, Minibatch Loss=0.9833, Training Accuracy=0.828\n",
      "Step 1500, Minibatch Loss=0.6447, Training Accuracy=0.891\n",
      "Step 1600, Minibatch Loss=1.2576, Training Accuracy=0.828\n",
      "Step 1700, Minibatch Loss=0.5650, Training Accuracy=0.875\n",
      "Step 1800, Minibatch Loss=0.9057, Training Accuracy=0.906\n",
      "Step 1900, Minibatch Loss=0.8011, Training Accuracy=0.875\n",
      "Step 2000, Minibatch Loss=0.4509, Training Accuracy=0.938\n",
      "Step 2100, Minibatch Loss=0.7009, Training Accuracy=0.906\n",
      "Step 2200, Minibatch Loss=0.3362, Training Accuracy=0.922\n",
      "Step 2300, Minibatch Loss=0.6199, Training Accuracy=0.891\n",
      "Step 2400, Minibatch Loss=0.4851, Training Accuracy=0.875\n",
      "Reloading the iterator as epoch is finished\n",
      "Validation dataset is over\n",
      "Step VALIDATION_STEP, Minibatch Loss=0.6225, Training Accuracy=0.896\n",
      "Step 2500, Minibatch Loss=0.2240, Training Accuracy=0.969\n",
      "Step 2600, Minibatch Loss=0.4312, Training Accuracy=0.922\n",
      "Step 2700, Minibatch Loss=0.0788, Training Accuracy=0.969\n",
      "Step 2800, Minibatch Loss=0.5019, Training Accuracy=0.891\n",
      "Step 2900, Minibatch Loss=0.3408, Training Accuracy=0.922\n",
      "Step 3000, Minibatch Loss=0.2933, Training Accuracy=0.953\n",
      "Step 3100, Minibatch Loss=0.3572, Training Accuracy=0.922\n",
      "Step 3200, Minibatch Loss=0.5440, Training Accuracy=0.906\n",
      "Step 3300, Minibatch Loss=0.7490, Training Accuracy=0.891\n",
      "Step 3400, Minibatch Loss=0.3840, Training Accuracy=0.922\n",
      "Step 3500, Minibatch Loss=0.7576, Training Accuracy=0.875\n",
      "Step 3600, Minibatch Loss=0.3222, Training Accuracy=0.922\n",
      "Step 3700, Minibatch Loss=0.3820, Training Accuracy=0.938\n",
      "Step 3800, Minibatch Loss=0.1478, Training Accuracy=0.938\n",
      "Step 3900, Minibatch Loss=0.5075, Training Accuracy=0.891\n",
      "Step 4000, Minibatch Loss=1.1281, Training Accuracy=0.828\n",
      "Step 4100, Minibatch Loss=0.1322, Training Accuracy=0.953\n",
      "Step 4200, Minibatch Loss=0.1888, Training Accuracy=0.922\n",
      "Step 4300, Minibatch Loss=0.6244, Training Accuracy=0.906\n",
      "Step 4400, Minibatch Loss=0.7613, Training Accuracy=0.859\n",
      "Step 4500, Minibatch Loss=0.4530, Training Accuracy=0.922\n",
      "Step 4600, Minibatch Loss=0.4742, Training Accuracy=0.922\n",
      "Step 4700, Minibatch Loss=0.3218, Training Accuracy=0.906\n",
      "Step 4800, Minibatch Loss=0.6034, Training Accuracy=0.906\n",
      "Step 4900, Minibatch Loss=0.7238, Training Accuracy=0.828\n",
      "Reloading the iterator as epoch is finished\n",
      "Validation dataset is over\n",
      "Step VALIDATION_STEP, Minibatch Loss=0.7369, Training Accuracy=0.898\n",
      "Step 5000, Minibatch Loss=0.1990, Training Accuracy=0.984\n",
      "Step 5100, Minibatch Loss=0.1190, Training Accuracy=0.969\n",
      "Step 5200, Minibatch Loss=0.5521, Training Accuracy=0.844\n",
      "Step 5300, Minibatch Loss=0.3966, Training Accuracy=0.938\n",
      "Step 5400, Minibatch Loss=0.0575, Training Accuracy=0.984\n",
      "Step 5500, Minibatch Loss=0.3268, Training Accuracy=0.922\n",
      "Step 5600, Minibatch Loss=0.3938, Training Accuracy=0.969\n",
      "Step 5700, Minibatch Loss=0.5198, Training Accuracy=0.906\n",
      "Step 5800, Minibatch Loss=0.2646, Training Accuracy=0.938\n",
      "Step 5900, Minibatch Loss=1.2851, Training Accuracy=0.875\n",
      "Step 6000, Minibatch Loss=0.1730, Training Accuracy=0.969\n",
      "Step 6100, Minibatch Loss=0.0793, Training Accuracy=0.969\n",
      "Step 6200, Minibatch Loss=0.1996, Training Accuracy=0.953\n",
      "Step 6300, Minibatch Loss=0.0789, Training Accuracy=0.984\n",
      "Step 6400, Minibatch Loss=0.4211, Training Accuracy=0.922\n",
      "Step 6500, Minibatch Loss=0.3732, Training Accuracy=0.891\n",
      "Step 6600, Minibatch Loss=0.0910, Training Accuracy=0.984\n",
      "Step 6700, Minibatch Loss=0.3911, Training Accuracy=0.922\n",
      "Step 6800, Minibatch Loss=0.3944, Training Accuracy=0.938\n",
      "Step 6900, Minibatch Loss=0.3830, Training Accuracy=0.938\n",
      "Step 7000, Minibatch Loss=0.1345, Training Accuracy=0.969\n",
      "Step 7100, Minibatch Loss=0.0689, Training Accuracy=0.969\n",
      "Step 7200, Minibatch Loss=0.2300, Training Accuracy=0.938\n",
      "Step 7300, Minibatch Loss=0.5193, Training Accuracy=0.906\n",
      "Step 7400, Minibatch Loss=0.2456, Training Accuracy=0.922\n",
      "Reloading the iterator as epoch is finished\n",
      "Validation dataset is over\n",
      "Step VALIDATION_STEP, Minibatch Loss=0.8556, Training Accuracy=0.902\n",
      "Step 7500, Minibatch Loss=0.1923, Training Accuracy=0.969\n",
      "Step 7600, Minibatch Loss=0.2436, Training Accuracy=0.938\n",
      "Step 7700, Minibatch Loss=0.3266, Training Accuracy=0.953\n",
      "Step 7800, Minibatch Loss=0.0819, Training Accuracy=0.984\n",
      "Step 7900, Minibatch Loss=0.2933, Training Accuracy=0.922\n",
      "Step 8000, Minibatch Loss=0.1063, Training Accuracy=0.969\n",
      "Step 8100, Minibatch Loss=0.5022, Training Accuracy=0.906\n",
      "Step 8200, Minibatch Loss=0.3481, Training Accuracy=0.906\n",
      "Step 8300, Minibatch Loss=0.1774, Training Accuracy=0.969\n",
      "Step 8400, Minibatch Loss=0.0299, Training Accuracy=0.984\n",
      "Step 8500, Minibatch Loss=0.0523, Training Accuracy=0.969\n",
      "Step 8600, Minibatch Loss=0.4509, Training Accuracy=0.938\n",
      "Step 8700, Minibatch Loss=0.3614, Training Accuracy=0.938\n",
      "Step 8800, Minibatch Loss=0.6351, Training Accuracy=0.891\n",
      "Step 8900, Minibatch Loss=0.1475, Training Accuracy=0.984\n",
      "Step 9000, Minibatch Loss=0.5041, Training Accuracy=0.938\n",
      "Step 9100, Minibatch Loss=0.3587, Training Accuracy=0.891\n",
      "Step 9200, Minibatch Loss=0.4587, Training Accuracy=0.953\n",
      "Step 9300, Minibatch Loss=0.1354, Training Accuracy=0.953\n",
      "Step 9400, Minibatch Loss=0.2907, Training Accuracy=0.953\n",
      "Step 9500, Minibatch Loss=0.0855, Training Accuracy=0.984\n",
      "Step 9600, Minibatch Loss=0.3142, Training Accuracy=0.922\n",
      "Step 9700, Minibatch Loss=0.2846, Training Accuracy=0.953\n",
      "Step 9800, Minibatch Loss=0.6713, Training Accuracy=0.922\n",
      "Reloading the iterator as epoch is finished\n",
      "Validation dataset is over\n",
      "Step VALIDATION_STEP, Minibatch Loss=1.0388, Training Accuracy=0.906\n",
      "Step 9900, Minibatch Loss=0.0580, Training Accuracy=0.969\n",
      "Step 10000, Minibatch Loss=0.1516, Training Accuracy=0.984\n",
      "Step 10100, Minibatch Loss=0.3518, Training Accuracy=0.938\n",
      "Step 10200, Minibatch Loss=0.1846, Training Accuracy=0.938\n",
      "Step 10300, Minibatch Loss=0.0417, Training Accuracy=0.984\n",
      "Step 10400, Minibatch Loss=0.1206, Training Accuracy=0.969\n",
      "Step 10500, Minibatch Loss=0.0608, Training Accuracy=0.969\n",
      "Step 10600, Minibatch Loss=0.1685, Training Accuracy=0.938\n",
      "Step 10700, Minibatch Loss=0.2775, Training Accuracy=0.922\n",
      "Step 10800, Minibatch Loss=0.2862, Training Accuracy=0.906\n",
      "Step 10900, Minibatch Loss=0.1098, Training Accuracy=0.953\n",
      "Step 11000, Minibatch Loss=0.2370, Training Accuracy=0.953\n",
      "Step 11100, Minibatch Loss=0.4311, Training Accuracy=0.922\n",
      "Step 11200, Minibatch Loss=0.2958, Training Accuracy=0.938\n",
      "Step 11300, Minibatch Loss=0.0537, Training Accuracy=0.969\n",
      "Step 11400, Minibatch Loss=0.3064, Training Accuracy=0.922\n",
      "Step 11500, Minibatch Loss=0.4062, Training Accuracy=0.922\n",
      "Step 11600, Minibatch Loss=0.4867, Training Accuracy=0.938\n",
      "Step 11700, Minibatch Loss=0.4025, Training Accuracy=0.922\n",
      "Step 11800, Minibatch Loss=0.3317, Training Accuracy=0.922\n",
      "Step 11900, Minibatch Loss=0.1560, Training Accuracy=0.969\n",
      "Step 12000, Minibatch Loss=0.1155, Training Accuracy=0.953\n",
      "Step 12100, Minibatch Loss=0.1805, Training Accuracy=0.953\n",
      "Step 12200, Minibatch Loss=0.1842, Training Accuracy=0.984\n",
      "Step 12300, Minibatch Loss=0.0220, Training Accuracy=1.000\n",
      "Reloading the iterator as epoch is finished\n",
      "Validation dataset is over\n",
      "Step VALIDATION_STEP, Minibatch Loss=1.1486, Training Accuracy=0.908\n",
      "Training is Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "NUM_EPOCH=5\n",
    "DISPLAY_STEP = 100\n",
    "epoch = 0\n",
    "step = 0\n",
    "while epoch < NUM_EPOCH:\n",
    "    try:\n",
    "        # Run optimization\n",
    "        sess.run(train_op, feed_dict={is_training: True})\n",
    "        step = step + 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print (\"Reloading the iterator as epoch is finished\")\n",
    "        validation(sess, val_data, val_label)\n",
    "        epoch = epoch + 1\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: train_data, labels_placeholder: train_label})\n",
    "\n",
    "    if step % DISPLAY_STEP == 0 or step == 1:\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={is_training: True})\n",
    "        print_progress(step, loss, acc)\n",
    "\n",
    "print(\"Training is Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"saved_models/cnn_level4/version1.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " def save_weights(saver, sess, path):\n",
    "    save_path = saver.save(sess, path)\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: saved_models/cnn_level4/version1.ckpt\n"
     ]
    }
   ],
   "source": [
    "save_weights(saver, sess, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_weights(saver, sess, path):\n",
    "    saver.restore(sess, path)\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_models/cnn_level3/version1.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "restore_weights(saver, sess, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.layers.dense(inputs=dropout, units=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
