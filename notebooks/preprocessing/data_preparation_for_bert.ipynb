{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from common.bio.amino_acid import *\n",
    "from common.preprocessing.dataframe import *\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This kernel has  4 cores and you can find the information regarding the memory usage: svmem(total=8438435840, available=2001723392, percent=76.3, used=6436712448, free=2001723392)\n"
     ]
    }
   ],
   "source": [
    "num_cores = mp.cpu_count()\n",
    "print(\"This kernel has \",num_cores,\"cores and you can find the information regarding the memory usage:\",psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../../data/protein/embedding/\"\n",
    "DATA_SOURCE = ROOT + \"data_sources/all_sequences.tsv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(DATA_SOURCE, sep='\\t', chunksize = 500000, skipinitialspace=True, usecols= [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"{}/{}/\".format(ROOT,\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_tfrecords(filename, data, options, extension=\"tfrecords.gz\"):\n",
    "    try:\n",
    "        filename = \"{}.{}\".format(filename, extension)\n",
    "        with tf.python_io.TFRecordWriter(filename,options) as writer:\n",
    "            for row in data:\n",
    "                example = tf.train.Example(features = tf.train.Features(\n",
    "                    feature={\n",
    "                        'length': tf.train.Feature(int64_list=tf.train.Int64List(value=[len(row)])),\n",
    "                        'seq': tf.train.Feature(int64_list=tf.train.Int64List(value=row))\n",
    "                    }\n",
    "                ))\n",
    "                writer.write(example.SerializeToString())\n",
    "        print(\"Data was stored in {}\".format(filename))\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong went writting in to tfrecords file\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_tfrecords_multithreaded(path, original_data):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    threading_start = time.time()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = []\n",
    "    options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    for i, data in enumerate(original_data):\n",
    "        filename = os.path.join(path, str(i))\n",
    "        data = filter_non_standard_amino_acids(data, \"Sequence\")\n",
    "        data = data[\"Sequence\"].apply(lambda x: [ AMINO_ACID_TO_ID[c] for c in x])\n",
    "        args = (filename, data, options)\n",
    "        print(\"Completed all threads in {} seconds\".format(time.time() - threading_start))\n",
    "        t = threading.Thread(target=save_as_tfrecords, args=args)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "        if i == 0:\n",
    "            break\n",
    "    coord.join(threads)\n",
    "    print(\"Completed all threads in {} seconds\".format(time.time() - threading_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed all threads in 19.44636106491089 seconds\n",
      "Data was stored in ../../data/protein/embedding//all/0.tfrecords\n",
      "Completed all threads in 87.57551789283752 seconds\n"
     ]
    }
   ],
   "source": [
    "save_as_tfrecords_multithreaded(PATH, original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.data.TFRecordDataset(filenames=filenames, compression_type='GZIP', buffer_size=buffer_size)\n",
    "# features = tf.parse_single_example(\n",
    "#     serialized=example.SerializeToString(),\n",
    "#     features={\n",
    "#         'label': tf.FixedLenFeature([1], tf.int64),\n",
    "#         'seq': tf.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
